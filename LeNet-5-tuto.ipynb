{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 28\n",
      "Rows: 10000,clumns: 28\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100 \n",
    "def load_mnist(path,kind='train'):\n",
    "    labels_path = os.path.join(path,'%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path,'%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path,'rb') as lbpath:\n",
    "        magic,n = struct.unpack('>II',lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,dtype = np.uint8)\n",
    "    \n",
    "    with open(images_path,'rb') as imgpath:\n",
    "        magic,num,rows,cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels),IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS)\n",
    "        \n",
    "    return images,labels\n",
    "\n",
    "#load data\n",
    "train_data,train_labels = load_mnist('mnist',kind='train')\n",
    "print('Rows: %d, columns: %d'%(train_data.shape[0],train_data.shape[1]))\n",
    "test_data,test_labels = load_mnist('mnist',kind ='t10k' )\n",
    "print('Rows: %d,clumns: %d' %(test_data.shape[0],test_data.shape[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = train_data[:VALIDATION_SIZE,...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:,...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a 2D convolution in Tensorflow\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_eager_execution()\n",
    "#tf.nn.conv2d(input,filter,strides=[1,1,1,1],padding='SAME',use_cudnn_on_gpu=None,data_format=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#defining learnable weights for the convolutional layers\n",
    "conv1_weights = tf.Variable(tf.truncated_normal([5,5,NUM_CHANNELS,32],stddev=0.1,dtype = tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([32],dtype = tf.float32))\n",
    "conv2_weights = tf.Variable(tf.truncated_normal([5,5,32,64],stddev = 0.1,seed=SEED,dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1,shape=[64],dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining learnable weights for the fully connected layers\n",
    "fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64,512],stddev=0.1,seed=SEED,dtype = tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1,shape=[512],dtype = tf.float32))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512,NUM_LABELS],stddev=0.1,seed=SEED,dtype = tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1,shape=[NUM_LABELS],dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet-5 architecture\n",
    "def model(data,train=False):\n",
    "    conv = tf.nn.conv2d(data,conv1_weights,strides=[1,1,1,1],padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv,conv1_biases))\n",
    "    pool = tf.nn.max_pool(relu,ksize= [1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,conv2_weights,strides=[1,1,1,1],padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv,conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    #reshape the feauture map cuboid into a 2D matrix to feed it the fully connected layers\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool,[pool_shape[0],pool_shape[1]*pool_shape[2]*pool_shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    #add 50% dropout during training\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden,0.5,seed=SEED)\n",
    "    return tf.matmul(hidden,fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining placholders\n",
    "train_data_node = tf.placeholder(tf.float32,shape=(BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64,shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(tf.float32,shape=(EVAL_BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_in_batches(data,sess):\n",
    "    #get predictions for a dataset by running it in small batches\n",
    "    size = data.shape[0]\n",
    "    if size <EVAL_BATCH_SIZE:\n",
    "        raise ValueError('batch size for evals larger than dataset: %d', size)\n",
    "    predictions = numpy.ndarray(shape=(size,NUM_LABELS),dtype=numpy.float32)\n",
    "    for begin in range(0,size,EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end,:] = sess.run(eval_prediction,feed_dict={eval_data: data[begin:end, ...]})\n",
    "        else:\n",
    "            batch_predictions = sess.run(eval_prediction,feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "            predictions[begin:,:] = batch_predictions[begin - size:,:]\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def error_rate(predictions,labels):\n",
    "    return 100.0 - (100.0 * numpy.sum(numpy.argmax(predictions,1)==labels)/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights)\n",
    "                + tf.nn.l2_loss(fc1_biases)\n",
    "                + tf.nn.l2_loss(fc2_weights)\n",
    "                + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0, dtype=tf.float32)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(0.01,batch* BATCH_SIZE,train_size, 0.95, staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss,global_step=batch)\n",
    "\n",
    "# Predictions for the current training minibatch.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Predictions for the test and validation, which we'll compute less\n",
    "# often.\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00), 4.1 ms\n",
      "Minibatch loss: 276522.531, learning rate: 0.010000\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.9%\n",
      "Step 100 (epoch 0.12), 84.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.9%\n",
      "Step 200 (epoch 0.23), 82.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 96.9%\n",
      "Validation error :90.9%\n",
      "Step 300 (epoch 0.35), 84.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.9%\n",
      "Step 400 (epoch 0.47), 81.4 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.9%\n",
      "Step 500 (epoch 0.58), 89.8 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.9%\n",
      "Step 600 (epoch 0.70), 89.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.9%\n",
      "Step 700 (epoch 0.81), 83.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.3%\n",
      "Step 800 (epoch 0.93), 85.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.010000\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.2%\n",
      "Step 900 (epoch 1.05), 90.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.2%\n",
      "Step 1000 (epoch 1.16), 83.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.7%\n",
      "Step 1100 (epoch 1.28), 86.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.6%\n",
      "Step 1200 (epoch 1.40), 85.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 95.3%\n",
      "Validation error :89.9%\n",
      "Step 1300 (epoch 1.51), 85.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.7%\n",
      "Step 1400 (epoch 1.63), 86.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.2%\n",
      "Step 1500 (epoch 1.75), 87.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 92.2%\n",
      "Validation error :89.6%\n",
      "Step 1600 (epoch 1.86), 83.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.6%\n",
      "Step 1700 (epoch 1.98), 91.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009500\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.4%\n",
      "Step 1800 (epoch 2.09), 86.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 84.4%\n",
      "Validation error :89.9%\n",
      "Step 1900 (epoch 2.21), 82.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.4%\n",
      "Step 2000 (epoch 2.33), 85.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.3%\n",
      "Step 2100 (epoch 2.44), 82.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 84.4%\n",
      "Validation error :90.8%\n",
      "Step 2200 (epoch 2.56), 82.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.3%\n",
      "Step 2300 (epoch 2.68), 82.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.3%\n",
      "Step 2400 (epoch 2.79), 81.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.0%\n",
      "Step 2500 (epoch 2.91), 81.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.009025\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.1%\n",
      "Step 2600 (epoch 3.03), 84.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 82.8%\n",
      "Validation error :90.2%\n",
      "Step 2700 (epoch 3.14), 85.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.4%\n",
      "Step 2800 (epoch 3.26), 86.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.1%\n",
      "Step 2900 (epoch 3.37), 89.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.2%\n",
      "Step 3000 (epoch 3.49), 84.8 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 93.8%\n",
      "Validation error :91.0%\n",
      "Step 3100 (epoch 3.61), 84.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.7%\n",
      "Step 3200 (epoch 3.72), 82.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.9%\n",
      "Step 3300 (epoch 3.84), 83.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.6%\n",
      "Step 3400 (epoch 3.96), 90.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.008574\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.2%\n",
      "Step 3500 (epoch 4.07), 83.4 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.6%\n",
      "Step 3600 (epoch 4.19), 87.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.8%\n",
      "Step 3700 (epoch 4.31), 84.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 96.9%\n",
      "Validation error :90.8%\n",
      "Step 3800 (epoch 4.42), 84.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.2%\n",
      "Step 3900 (epoch 4.54), 87.6 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.1%\n",
      "Step 4000 (epoch 4.65), 85.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.3%\n",
      "Step 4100 (epoch 4.77), 89.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.5%\n",
      "Step 4200 (epoch 4.89), 84.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.008145\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.7%\n",
      "Step 4300 (epoch 5.00), 91.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.3%\n",
      "Step 4400 (epoch 5.12), 88.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.9%\n",
      "Step 4500 (epoch 5.24), 82.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 93.8%\n",
      "Validation error :89.8%\n",
      "Step 4600 (epoch 5.35), 87.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.5%\n",
      "Step 4700 (epoch 5.47), 84.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 85.9%\n",
      "Validation error :89.8%\n",
      "Step 4800 (epoch 5.59), 94.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.3%\n",
      "Step 4900 (epoch 5.70), 95.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.8%\n",
      "Step 5000 (epoch 5.82), 93.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 87.5%\n",
      "Validation error :89.8%\n",
      "Step 5100 (epoch 5.93), 94.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.007738\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.5%\n",
      "Step 5200 (epoch 6.05), 86.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.1%\n",
      "Step 5300 (epoch 6.17), 83.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.0%\n",
      "Step 5400 (epoch 6.28), 85.8 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 96.9%\n",
      "Validation error :90.3%\n",
      "Step 5500 (epoch 6.40), 84.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 82.8%\n",
      "Validation error :90.1%\n",
      "Step 5600 (epoch 6.52), 84.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.1%\n",
      "Step 5700 (epoch 6.63), 83.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 82.8%\n",
      "Validation error :90.2%\n",
      "Step 5800 (epoch 6.75), 87.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.3%\n",
      "Step 5900 (epoch 6.87), 89.6 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.1%\n",
      "Step 6000 (epoch 6.98), 92.4 ms\n",
      "Minibatch loss: nan, learning rate: 0.007351\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.5%\n",
      "Step 6100 (epoch 7.10), 97.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 92.2%\n",
      "Validation error :89.8%\n",
      "Step 6200 (epoch 7.21), 85.8 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 95.3%\n",
      "Validation error :89.7%\n",
      "Step 6300 (epoch 7.33), 85.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.3%\n",
      "Step 6400 (epoch 7.45), 87.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.2%\n",
      "Step 6500 (epoch 7.56), 89.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.4%\n",
      "Step 6600 (epoch 7.68), 88.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6700 (epoch 7.80), 91.4 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.9%\n",
      "Step 6800 (epoch 7.91), 87.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.006983\n",
      "Minibatch error: 85.9%\n",
      "Validation error :90.6%\n",
      "Step 6900 (epoch 8.03), 85.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.1%\n",
      "Step 7000 (epoch 8.15), 86.2 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.2%\n",
      "Step 7100 (epoch 8.26), 88.6 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.0%\n",
      "Step 7200 (epoch 8.38), 85.6 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.1%\n",
      "Step 7300 (epoch 8.49), 88.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.3%\n",
      "Step 7400 (epoch 8.61), 89.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 92.2%\n",
      "Validation error :90.1%\n",
      "Step 7500 (epoch 8.73), 86.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.2%\n",
      "Step 7600 (epoch 8.84), 90.3 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 95.3%\n",
      "Validation error :90.2%\n",
      "Step 7700 (epoch 8.96), 82.8 ms\n",
      "Minibatch loss: nan, learning rate: 0.006634\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.4%\n",
      "Step 7800 (epoch 9.08), 93.4 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.3%\n",
      "Step 7900 (epoch 9.19), 90.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.9%\n",
      "Step 8000 (epoch 9.31), 88.7 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 84.4%\n",
      "Validation error :90.7%\n",
      "Step 8100 (epoch 9.43), 90.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 93.8%\n",
      "Validation error :90.5%\n",
      "Step 8200 (epoch 9.54), 84.9 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 90.6%\n",
      "Validation error :90.0%\n",
      "Step 8300 (epoch 9.66), 84.1 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 87.5%\n",
      "Validation error :90.5%\n",
      "Step 8400 (epoch 9.77), 96.0 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 84.4%\n",
      "Validation error :90.1%\n",
      "Step 8500 (epoch 9.89), 97.5 ms\n",
      "Minibatch loss: nan, learning rate: 0.006302\n",
      "Minibatch error: 89.1%\n",
      "Validation error :90.2%\n",
      "Test error: 90.1%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(int(NUM_EPOCHS*train_size)// BATCH_SIZE):\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE),...]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        feed_dict = {train_data_node: batch_data,train_labels_node:batch_labels}\n",
    "        #run the optimizer to update weights\n",
    "        sess.run(optimizer,feed_dict=feed_dict)\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            l,lr,predictions = sess.run([loss,learning_rate,train_prediction],feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size,1000*elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss: %.3f, learning rate: %.6f' %(l,lr))\n",
    "            print('Minibatch error: %.1f%%' % error_rate(predictions,batch_labels))\n",
    "            print('Validation error :%.1f%%' % error_rate(eval_in_batches(validation_data,sess),validation_labels))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \n",
    "    test_error = error_rate(eval_in_batches(test_data,sess),test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
